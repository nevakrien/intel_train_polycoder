{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda76b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import transformers\n",
    "from transformers import GPTNeoXForCausalLM, GPTNeoXConfig, GPT2Tokenizer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c02e8ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training params\n",
    "num_iters = 10\n",
    "eval_interval = 5  \n",
    "save_interval = 10 \n",
    "checkpoint_dir = './checkpoints' \n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    \n",
    "model_dir='./configs/150m'\n",
    "max_len=30#None #none for setting it by model config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6a909",
   "metadata": {},
   "source": [
    "# model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77990843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prev_model = GPTNeoXForCausalLM.from_pretrained(\"NinedayWang/PolyCoder-2.7B\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"NinedayWang/PolyCoder-2.7B\")\n",
    "\n",
    "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c64eb337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config=prev_model.config\n",
    "#config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a45a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config.save_pretrained('./configs/2.7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35e2d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.save_pretrained('./configs/tokenizer')\n",
    "#tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8227932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=768, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=GPT2Tokenizer.from_pretrained('./configs/tokenizer')\n",
    "config=GPTNeoXConfig.from_pretrained(model_dir)\n",
    "model=GPTNeoXForCausalLM(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d430ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if max_len==None:\n",
    "    max_len=config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4872b55",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4af32b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_file='cpp000000000302.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b873a3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: 10108 errors: 0\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "errors=[]\n",
    "faultys=[]\n",
    "with open(data_file,'rb') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        try:\n",
    "          data.append(json.loads(line))\n",
    "        except Exception as e:\n",
    "          print(f'errored at {i}')\n",
    "          errors.append(e)\n",
    "          faultys.append(line)\n",
    "\n",
    "# Now 'data' is a list of all the JSON objects in the file\n",
    "print(f'data: {len(data)} errors: {len(errors)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff7f6bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes=[d['content'] for d in data[0:100] if 'content' in d.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6194ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        for text in tqdm(texts):\n",
    "            encodings = tokenizer(text, truncation=True,\n",
    "                                  #padding='max_length',\n",
    "                                  max_length=max_len)\n",
    "            #encodings=tokenizer(text)\n",
    "            self.inputs.append(encodings['input_ids'])\n",
    "            self.targets.append(encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"input_ids\": torch.IntTensor(self.inputs[idx]), \n",
    "                \"labels\": torch.IntTensor(self.targets[idx])}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e87f1d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17089b299a2b46c0b9eb03faa1127808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7fcda0262320>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize your dataset and create a PyTorch Dataset\n",
    "# Assuming `codes` is a list of strings\n",
    "dataset = TextDataset(codes, tokenizer)\n",
    "\n",
    "# Split dataset into training and test set\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f475a9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b019dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c5508ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True)\n",
    "    labels = pad_sequence([item['labels'] for item in batch], batch_first=True)\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f121b2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([16, 30]), 'labels': torch.Size([16, 30])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    x=batch\n",
    "    break\n",
    "    \n",
    "{k:v.shape for k,v in x.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d31adb0",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "908851b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device)\n",
    "model=torch.nn.DataParallel(model, device_ids=['cpu'])\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00016, betas=(0.9, 0.999), eps=1.0e-8)\n",
    "\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=num_iters, T_mult=1, eta_min=0, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "beb0826d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611c0b9c164040ec92bb238b9c9d7292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 8.92725165685018\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b005c1b1db7c44a9874912e8abcc2fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 7.505497694015503\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113dab5ca93b41f69dcf0be85eb66d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 6.513497829437256\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1c260015614143bf24a73aee069bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 5.762369712193807\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399a0305f5dd44168aac7dd8ddfe427e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 4.894678036371867\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9868c855ed2f4a8b98a95d9fb4ddd74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 4.205408215522766\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b957ff7b52214a7da34ec5ed5d9132f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 3.3666664759318032\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12135b65f4d4447d84d35fb9dbb10c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 2.965507745742798\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e44c785f2fd4390aac059c03336ed3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 2.5062397718429565\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efc3543a6f34d2b99956af0668aa9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 2.0898282329241433\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8279c85e034272a403a2f15b85477e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average evaluation loss: 4.286717891693115\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "# Training Loop\n",
    "for epoch in range(1, num_iters+1):\n",
    "    print(f'Epoch {epoch}/{num_iters}')\n",
    "    \n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training loop with tqdm\n",
    "    train_loader_tqdm = tqdm(train_loader)\n",
    "    for batch in train_loader_tqdm:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Load data and labels\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device).to(torch.long)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        \n",
    "        # Get the loss from the outputs\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "        # Add the loss to the total loss\n",
    "        total_loss += loss.cpu().detach().item()\n",
    "\n",
    "        # Update the progress bar\n",
    "        train_loader_tqdm.set_postfix({'running_loss': total_loss /  (train_loader_tqdm.n + 1)})\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Average training loss: {avg_train_loss}\")\n",
    "\n",
    "    # Evaluation\n",
    "    # Evaluation\n",
    "if epoch % eval_interval == 0:\n",
    "    model.eval()\n",
    "    eval_total_loss = 0\n",
    "\n",
    "    # Adding tqdm to evaluation loop\n",
    "    test_loader_tqdm = tqdm(test_loader, desc=\"Evaluating\")\n",
    "    for batch in test_loader_tqdm:\n",
    "        with torch.no_grad():\n",
    "            # Load data and labels\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device).to(torch.long)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, labels=labels)\n",
    "            \n",
    "            # Get the loss from the outputs\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Add the loss to the total loss\n",
    "            eval_total_loss += loss.cpu().detach().item()\n",
    "\n",
    "            # Update the progress bar\n",
    "            test_loader_tqdm.set_postfix({'eval_loss': eval_total_loss / (test_loader_tqdm.n + 1)})\n",
    "\n",
    "    avg_eval_loss = eval_total_loss / len(test_loader)\n",
    "    print(f\"Average evaluation loss: {avg_eval_loss}\")\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    # Save a checkpoint\n",
    "    if epoch % save_interval == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_train_loss,\n",
    "        }, f'{checkpoint_dir}/checkpoint_{epoch}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c4988b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a checkpoint\n",
    "checkpoint = torch.load(f'{checkpoint_dir}/checkpoint_{epoch}.pt')\n",
    "\n",
    "# Load the checkpoint into your model and optimizer\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a9f5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
